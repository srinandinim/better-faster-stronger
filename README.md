# better-faster-stronger
*Completed in collaboration between Tej Shah, Shivam Agrawal, & Srinandini Marpaka*

# Abstract
Using the same Circle of Life environment from Project 2, we build a few intelligent agents for different environments. In the complete information environment, we use Dynamic Programming and Value Iteration to iteratively solve for the Bellman Equations to calculate optimal utilities for every state in the state space. Using those utilities $U^*$, we build an agent with the optimal policy using the Bellman Equations formulation. For data efficiency, we develop and implement a neural network function approximator $V$ from scratch to approximate $$U^{*}$$ which can be queried at inference time for making decisions. In the partial prey information setting, we develop $$U_{partial}$$ which is the expected value of all the optimal utilities $U^*$ for every location of prey using our probability distribution $$\textbf{p}_{prey}$$. For data efficiency, we develop and implement a generalized neural network function approximator $V_{partial}$ to predict $U_{partial}$ from data. Finally, we develop an optimal $$U^{*}_{partial}$$ using the temporal difference method of Deep-Q Learning, combining Value Iteration, Neural Networks, and Bellman Equations into one algorithm.